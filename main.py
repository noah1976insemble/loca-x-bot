# main.py
# loca-play.jp ã®ã‚«ãƒ†ã‚´ãƒªRSSã‚’èª­ã¿ã€æ–°ç€è¨˜äº‹ã®æœ¬æ–‡ã‚’æŠ½å‡ºã—ã¦
# GPT-3.5ã§Xå‘ã‘140å­—è¦ç´„ã‚’ä½œã‚‹æœ€å°ã‚µãƒ³ãƒ—ãƒ«

import os
import re
import textwrap
import feedparser
import requests
from bs4 import BeautifulSoup
from openai import OpenAI

from datetime import datetime, timezone, timedelta
import json
import time

# ====== è¨­å®š ======
FEED_URL = "https://loca-play.jp/essentials/feed/"   # â† å¿…è¦ã«å¿œã˜ã¦CPTã®RSSã«å¤‰æ›´
USER_AGENT = "loca-x-bot/0.1 (+https://loca-play.jp)"
MAX_FETCH = int(os.getenv("MAX_FETCH", "20"))  # ä¸€åº¦ã«å‡¦ç†ã™ã‚‹è¨˜äº‹æ•°ã®ä¸Šé™ï¼ˆæ–°ç€ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰

DATA_FILE = "data.json"   # æŠ•ç¨¿æ¸ˆã¿è¨˜äº‹ã®IDã‚’ä¿å­˜ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
MAX_AGE_HOURS = int(os.getenv("MAX_AGE_HOURS", "168"))  # ä½•æ™‚é–“ä»¥å†…ï¼ˆæ—¢å®š: 7æ—¥ä»¥å†…ï¼‰ã‚’å¯¾è±¡
# DRY_RUN ãƒ¢ãƒ¼ãƒ‰è¨­å®š:
# "none"         â†’ å®Ÿéš›ã«é€ä¿¡ã—ã¦è¨˜éŒ²ï¼ˆæœ¬ç•ªï¼‰
# "print-only"   â†’ é€ä¿¡ã›ãšprintã®ã¿ï¼ˆãƒ‡ãƒ¢ç”¨ã€è¨˜éŒ²ã‚‚æ®‹ã•ãªã„ï¼‰
# "record-only"  â†’ é€ä¿¡ã›ãšprintã—ã€è¨˜éŒ²ã ã‘æ®‹ã™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
DRY_RUN = "none"

# ====== OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ ======
# äº‹å‰ã«: export OPENAI_API_KEY="sk-xxxxx"
client = OpenAI()  # ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ã‚’è‡ªå‹•å‚ç…§

# ====== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ======
def load_posted_ids() -> set:
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            return set(json.load(f))
    except FileNotFoundError:
        return set()
    except Exception:
        return set()

def save_posted_ids(ids: set) -> None:
    try:
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted(list(ids))[:200], f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ æŠ•ç¨¿å±¥æ­´ã®ä¿å­˜ã«å¤±æ•—: {e}")

def entry_age_hours(entry) -> float:
    # feedparserã®published_parsedã‚’å„ªå…ˆã€‚ç„¡ã‘ã‚Œã°0æ™‚é–“æ‰±ã„ï¼ˆæ–°ã—ã„ã¨ã¿ãªã™ï¼‰
    if getattr(entry, "published_parsed", None):
        dt = datetime(*entry.published_parsed[:6], tzinfo=timezone.utc)
        return (datetime.now(timezone.utc) - dt).total_seconds() / 3600.0
    return 0.0

def fetch_feed(url: str):
    """RSSã‚’å–å¾—ã—ã¦ã‚¨ãƒ³ãƒˆãƒªä¸€è¦§ã‚’è¿”ã™"""
    feed = feedparser.parse(url)
    return feed.entries or []

def fetch_article_html(url: str) -> str:
    """è¨˜äº‹HTMLã‚’å–å¾—"""
    headers = {"User-Agent": USER_AGENT}
    resp = requests.get(url, headers=headers, timeout=20)
    resp.raise_for_status()
    return resp.text

def extract_main_text(html: str) -> str:
    """WordPressæƒ³å®šã§æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆæ±ç”¨çš„ã«ï¼‰"""
    soup = BeautifulSoup(html, "html.parser")

    # ä¸è¦è¦ç´ ã‚’é™¤å»
    for tag in soup(["script", "style", "noscript"]):
        tag.decompose()

    # ã‚ˆãã‚ã‚‹æœ¬æ–‡ã‚³ãƒ³ãƒ†ãƒŠã®å€™è£œ
    candidates = [
        {"name": "div", "class_": re.compile(r"(entry-content|post-content|content__body)")},
        {"name": "article"},
        {"name": "main"},
    ]

    for sel in candidates:
        node = soup.find(sel.get("name"), class_=sel.get("class_"))
        if node:
            text = node.get_text(separator="\n", strip=True)
            if len(text) > 200:  # ã‚ã‚‹ç¨‹åº¦ã®é•·ã•ãŒã‚ã‚‹ãªã‚‰æœ¬æ–‡ã¨ã¿ãªã™
                return text

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒšãƒ¼ã‚¸å…¨ä½“ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    return soup.get_text(separator="\n", strip=True)

def summarize_for_x(title: str, text: str) -> str:
    """GPT-3.5ã§Xå‘ã‘140å­—è¦ç´„ã‚’ä½œã‚‹"""
    # å…¥åŠ›ã‚’é•·ã™ããªã„ã‚ˆã†ã«çŸ­ç¸®ï¼ˆæ—¥æœ¬èªã¯ã–ã£ãã‚Šã§OKï¼‰
    snippet = text[:2000]

    prompt = textwrap.dedent(f"""
    ã‚ãªãŸã¯ã‚½ãƒ¼ã‚·ãƒ£ãƒ«å‘ã‘è¦ç´„ã®é”äººã§ã™ã€‚
    æ¬¡ã®è¨˜äº‹å†…å®¹ã‚’ã€Xï¼ˆæ—§Twitterï¼‰ã«æŠ•ç¨¿ã™ã‚‹å‰æã§**æ—¥æœ¬èª140æ–‡å­—ä»¥å†…**ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚
    ã“ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã¯ã€ã€Œãƒ•ã‚¯ãƒ­ã‚¦ã®ãƒ­ã‚«ãƒ­ã‚¦ãã‚“ã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ãŒé‹å–¶ã—ã¦ã„ã¾ã™ã€‚
    ãƒ«ãƒ¼ãƒ«:
    - èªå°¾ã«ã€Œã€œã ã»ãƒ¼ã€ã€Œã€œã§ã™ã»ãƒ¼ã€ã‚’ã¤ã‘ã‚‹ï¼ˆãƒ•ã‚¯ãƒ­ã‚¦ã®é³´ãå£°ï¼‰
    - çµµæ–‡å­—ã¯1å€‹ã¾ã§
    - å®£ä¼ã£ã½ã•ã¯æ§ãˆã‚ã€è¦ç‚¹ã‚’ä¸€è¨€ã§
    - å›ºæœ‰åè©ã¨æ•°å­—ã¯ã§ãã‚‹ã ã‘æ®‹ã™
    - è¨˜äº‹ã‚’èª­ã¿ãŸããªã‚‹ã‚ˆã†ã«
    - **è¨˜äº‹å†…ã«ã‚ã‚‹ã€loca-play.jpä»¥å¤–ã®å¤–éƒ¨ã‚µã‚¤ãƒˆURLã¯æœ¬æ–‡ã«å«ã‚ãªã„**
    è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«: {title}
    æœ¬æ–‡æŠœç²‹:
    {snippet}
    """)

    res = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role":"user","content":prompt}],
        temperature=0.3,
    )
    summary = res.choices[0].message.content.strip()
    # å¤–éƒ¨URLãŒæ··å…¥ã—ãŸå ´åˆã«å‚™ãˆã€http/httpsã®URLã‚’é™¤å»ã—ã¦æ•´å½¢
    summary = re.sub(r"https?://\S+", "", summary)
    summary = re.sub(r"\s+", " ", summary).strip()

    # å¿µã®ãŸã‚140å­—ã«ä¸¸ã‚ã‚‹ï¼ˆè¶…ãˆãã†ãªå ´åˆï¼‰
    if len(summary) > 140:
        summary = summary[:138] + "â€¦"

    return summary

def post_to_ifttt(text: str) -> None:
    """IFTTT Webhook ã«æŠ•ç¨¿æœ¬æ–‡ã‚’é€ä¿¡ã™ã‚‹ã€‚ç’°å¢ƒå¤‰æ•° IFTTT_WEBHOOK_URL ã‚’ä½¿ç”¨ã€‚"""
    url = os.getenv("IFTTT_WEBHOOK_URL", "").strip()
    if not url:
        print("âš ï¸ IFTTT_WEBHOOK_URL ãŒæœªè¨­å®šã®ãŸã‚é€ä¿¡ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        return
    try:
        r = requests.post(url, json={"value1": text}, timeout=10)
        if r.ok:
            print("ğŸ½  IFTTTã¸é€ä¿¡ OK")
        else:
            print(f"âš ï¸ IFTTTé€ä¿¡ã‚¨ãƒ©ãƒ¼: {r.status_code} {r.text}")
    except Exception as e:
        print(f"âš ï¸ IFTTTé€ä¿¡ä¸­ã«ä¾‹å¤–: {e}")

# ====== ãƒ¡ã‚¤ãƒ³å‡¦ç† ======
def main():
    print("ğŸ§ª RSSã‚’å–å¾—:", FEED_URL)
    entries = fetch_feed(FEED_URL)
    posted_ids = load_posted_ids()
    if not entries:
        print("RSSã«ã‚¨ãƒ³ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # æ–°ç€ã‚’ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹: æœªæŠ•ç¨¿ ã‹ã¤ 7æ—¥ä»¥å†… ã‚’æŠ½å‡º
    eligible = []
    for idx, entry in enumerate(entries[:MAX_FETCH], start=1):
        title = getattr(entry, "title", "(no title)")
        link  = getattr(entry, "link", None)
        print(f"\n[{idx}] {title}")
        if not link:
            print("  â†’ URLãªã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        entry_id = getattr(entry, "id", link)
        if entry_id in posted_ids:
            print("  â†’ æ—¢ã«æŠ•ç¨¿æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
            continue

        age = entry_age_hours(entry)
        if age > MAX_AGE_HOURS:
            print(f"  â†’ å¤ã„è¨˜äº‹ï¼ˆ{age:.1f}hï¼‰ãªã®ã§ã‚¹ã‚­ãƒƒãƒ—")
            continue

        eligible.append(entry)

    if not eligible:
        print("\nğŸ“­ æ–°è¦ã«æŠ•ç¨¿ã™ã‚‹å¯¾è±¡ã¯ã‚ã‚Šã¾ã›ã‚“ï¼ˆæœªæŠ•ç¨¿ã‹ã¤æœŸé™å†…ã®ã‚¨ãƒ³ãƒˆãƒªãªã—ï¼‰ã€‚")
        return

    # å¤ã„é †â†’æ–°ã—ã„é †ã§æŠ•ç¨¿ï¼ˆæ™‚ç³»åˆ—ã‚’ä¿ã¤ï¼‰
    def _published_ts(e):
        pp = getattr(e, "published_parsed", None)
        return (datetime(*pp[:6], tzinfo=timezone.utc).timestamp() if pp else 0)

    eligible.sort(key=_published_ts)  # å¤ã„ã‚‚ã®ã‹ã‚‰

    posted_count = 0
    for entry in eligible:
        title = getattr(entry, "title", "(no title)")
        link  = getattr(entry, "link", None)
        entry_id = getattr(entry, "id", link)

        try:
            html = fetch_article_html(link)
            text = extract_main_text(html)
            if len(text) < 100:
                print(f"  â†’ æœ¬æ–‡ãŒçŸ­ã™ãã‚‹ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: {title}")
                continue

            summary = summarize_for_x(title, text)
            tweet_body = f"ã€æ–°ç€ã€‘{summary} {link}"
            if len(tweet_body) > 270:
                keep = 270 - len(link) - 1
                tweet_body = f"ã€æ–°ç€ã€‘{summary[:keep]}â€¦ {link}"

            print("\nğŸ§‚ è¦ç´„ï¼ˆXæŠ•ç¨¿æ¡ˆ)")
            print(tweet_body)

            if DRY_RUN == "print-only":
                print("\nğŸ§ª DRY_RUN=print-only â†’ é€ä¿¡ã›ãšã€è¨˜éŒ²ã‚‚æ®‹ã—ã¾ã›ã‚“")
            elif DRY_RUN == "record-only":
                print("\nğŸ§ª DRY_RUN=record-only â†’ é€ä¿¡ã›ãšã€è¨˜éŒ²ã ã‘æ®‹ã—ã¾ã™")
                posted_ids.add(entry_id)
                save_posted_ids(posted_ids)
                print("ğŸ“’ æŠ•ç¨¿å±¥æ­´ã‚’æ›´æ–°ã—ã¾ã—ãŸï¼ˆé‡è¤‡é˜²æ­¢ï¼‰")
            else:
                post_to_ifttt(tweet_body)
                posted_ids.add(entry_id)
                save_posted_ids(posted_ids)
                print("ğŸ½  æŠ•ç¨¿æ¸ˆã¿ã¨ã—ã¦è¨˜éŒ²ã—ã¾ã—ãŸï¼ˆé‡è¤‡é˜²æ­¢ï¼‰")
                time.sleep(2)  # é€£æŠ•ã«ãªã‚Šã™ããªã„ã‚ˆã†ã«é–“éš”ã‚’å°‘ã—ç©ºã‘ã‚‹

            posted_count += 1
        except Exception as e:
            print(f"  â†’ å¤±æ•—: {e}. æ¬¡ã®ã‚¨ãƒ³ãƒˆãƒªã‚’è©¦ã—ã¾ã™ã€‚")

    print(f"\nâœ… ã¾ã¨ã‚: {posted_count}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªã‚’å‡¦ç†ã—ã¾ã—ãŸï¼ˆãƒ¢ãƒ¼ãƒ‰: {DRY_RUN}, å¯¾è±¡æœŸé–“: {MAX_AGE_HOURS}hï¼‰ã€‚")

if __name__ == "__main__":
    main()